---
title: "hamnetfinder"
output: html_notebook
---

I was thinking it would be interested to try and develop a shiny app that
scrapes and presents ham nets with their times and frequencies and other 
information.

```{r get_spreadsheet}
library(readxl)
library(httr)

  get_todays_ham_nets <- function(zone) {
    # Gets today's xlsx file by time zone if it is not already on disk
    # Always returns the name of the data file
    thisDay <- Sys.Date()
    data_file <- paste0(zone,"_ham_nets_",thisDay,".Rda")
    if ( !file.exists(data_file) ) {
      xlsx_url <- paste0('https://www.theweatherwonder.com/Net_List_Spreadsheet_',zone,'_Time.xlsx')
      GET(xlsx_url, write_disk(tf <- tempfile(fileext = ".xlsx")))
      test <- read_excel(tf, sheet = 1)
      col_type_vec <- c("text", rep("guess", times=7), "date", "date", rep("text", times=3),"skip")
      xlsx <- read_excel(tf, sheet=1, col_types=col_type_vec[1:ncol(test)], skip =1)
      xlsx[,2:8] <- !is.na(xlsx[,2:8])
      xlsx[[zone]] <- format(xlsx[[zone]], format='%-I:%M %p')
      xlsx[["UTC"]] <- format(xlsx[["UTC"]], format='%-I:%M %p')
      xlsx[["Node"]] <- gsub("\\s", "", xlsx[["Node"]])
      save(xlsx, file=data_file)
    }
    data_file
  }

```

```{r}

load(file = get_todays_ham_nets("Pacific"))
xlsx

```
```{r}
library(httr)
library(rvest)
library(dplyr)


search_client_url <- "http://www.arrl.org/resources/nets/client/netsearch.html"
vals <- list(
  netype = "%L%",
  state = "Alabama",
  netname = "",
  dow = "%%",
  freq = "0-1000000",
  ntfs = "%%"
)
html <- read_html(search_client_url)
search <- html_form(html)[[1]] %>% html_form_set(!!!vals)
resp <- html_form_submit(search)
net_table <- read_html(resp) %>% 
  html_element("table") %>% 
  html_table() %>%
  select(-5) %>%
  rename("Local Days" = ends_with("Local Days")) %>%
  select("Local Time", "Net Name", "Frequency")
print(net_table)

```

